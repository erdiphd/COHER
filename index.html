<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="COHER Paper">
  <meta name="keywords" content="COHER">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>COHER</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./web/css/bulma.min.css">
  <link rel="stylesheet" href="./web/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./web/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./web/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./web/css/index.css">
  <link rel="icon" href="./web/images/robotic_arm.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./web/js/fontawesome.all.min.js"></script>
  <script src="./web/js/bulma-carousel.min.js"></script>
  <script src="./web/js/bulma-slider.min.js"></script>
  <script src="./web/js/index.js"></script>
  <script src="./web/js/mathjax-config.js"></script>



  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/monokai.min.css"
    crossorigin="anonymous" title="hl-light">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/monokai.min.css"
    crossorigin="anonymous" title="hl-dark" disabled>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/python.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/shell.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/dockerfile.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script>hljs.initHighlightingOnLoad();</script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/latex.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/tex.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full"
    integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://erdiphd.github.io/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Curriculum learning for robot manipulation tasks
              with sparse reward through environment shifts</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://erdisayar.github.io/">Erdi Sayar</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://sites.google.com/site/giovanniiacca/">Giovanni Iacca</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/">Alois
                  Knoll</a><sup>1</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Technical University of Munich,</span>
              <span class="author-block"><sup>2</sup>University of Trento</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://ieeexplore.ieee.org/document/10480429" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=nVuoxhVqBVw"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/erdiphd/COHER" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <iframe width="768" height="512" src="https://www.youtube.com/embed/nVuoxhVqBVw?si=DWxRKxyjG01nkISt"
          title="YouTube video player" frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen></iframe>
      </div>
    </div>
  </section>

  <h1>Sim2Real Videos</h1>

  <br />

  <div class="sim2real_youtube_videos_row">
    <div class="sim2real_youtube_videos_column">
      <h3 class="video-title">HGG</h3>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/tV8BbRbyv3k" title="YouTube video player"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen></iframe>
      </div>
    </div>


    <div class="sim2real_youtube_videos_column">
      <h3 class="video-title">COHER(ours)</h3>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/nVuoxhVqBVw" title="YouTube video player"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen></iframe>
      </div>
    </div>

    <div class="sim2real_youtube_videos_column">
      <h3 class="video-title">HGG</h3>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/fst53mOdC7A" title="YouTube video player"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen></iframe>
      </div>
    </div>

    <div class="sim2real_youtube_videos_column">
      <h3 class="video-title">COHER(ours)</h3>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/7poD_2C3cj0" title="YouTube video player"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen></iframe>
      </div>
    </div>
  </div>
  <div class="sim2real_youtube_videos_row">
    <div class="sim2real_youtube_videos_column">
      <h3 class="video-title">HGG</h3>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/2r-5qgLOluY" title="YouTube video player"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen></iframe>
      </div>
    </div>
    <div class="sim2real_youtube_videos_column">
      <h3 class="video-title">COHER(ours)</h3>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/jwKgE6RP5mY" title="YouTube video player"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen></iframe>
      </div>
    </div>
    <div class="sim2real_youtube_videos_column">
      <h3 class="video-title">HGG</h3>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/jTN0m1NsJt8" title="YouTube video player"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen></iframe>
      </div>
    </div>
    <div class="sim2real_youtube_videos_column">
      <h3 class="video-title">COHER(ours)</h3>
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/wEM0MBC7fpc" title="YouTube video player"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen></iframe>
      </div>
    </div>

  </div>



  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Multi-goal reinforcement learning (RL) problems with sparse rewards are challenging tasks. These can be
              solved by
              hindsight experience replay (HER), which learns from failures by replacing the achieved state of failed
              experiences
              with the desired goal. However, if the desired goals are far away from the initial states, HER becomes
              inefficient.
              To deal with this issue, curriculum-based RL approaches decompose complex tasks into sequences of
              gradually more
              difficult tasks, by relying on heuristics that guide the agent to explore the environment more
              efficiently. One
              possible alternative is to generate curricula in an open-ended way, i.e., by creating novel and
              increasingly more
              complex tasks without bounds. In this paper, we propose an extension of HER called co-adapting hindsight
              experience
              replay (COHER), inspired by the recently proposed Paired Open-Ended Trailblazer (POET). The generated
              tasks and
              agent are coupled to optimize the behavior of the agent within each task-agent pair. After reaching a
              predefined
              success rate, the next task is generated and the agent leverages the skills acquired from the prior task
              to solve
              the next (and more challenging) one. We evaluate COHER on various sparse reward robotic tasks that require
              obstacle
              avoidance capabilities, and compare COHER with hindsight goal generation (HGG), curriculum-guided
              hindsight
              experience replay (CHER), and vanilla HER. The results show that COHER consistently outperforms the other
              methods
              and that the obtained policies can avoid obstacles without having explicit information about their
              position. Lastly,
              we deploy such policies to a real Franka robot for Sim2Real analysis. We observe that the robot can
              achieve the task
              by avoiding obstacles, whereas policies obtained with other methods cannot. The videos and code are
              publicly
              available at: https://erdiphd.github.io/COHER/
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      <!-- Proposed Method. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Proposed method</h2>
          <div class="content has-text-justified">
            <p>
              In the following, we assume that we are working on a robot manipulation task in an environment with
              obstacles.
              Therefore, in the description of the proposed method we will refer to this specific task. Nevertheless,
              the method
              could be in principle extended to other kinds of tasks, provided that the environments can be
              characterized by
              different levels of difficulty.

              nspired by the POET methodology, our proposed method works as follows. We execute a
              curriculum learning process in which we maintain a population of environments $\mathcal{X}$ (each one
              characterized
              by a different number of obstacles, in different positions and with different sizes) and an agent
              $\mathcal{Y}$
              (i.e., a neural network). The environments can be generated either by the algorithm itself, or manually
              (as we
              actually do in the present study), and added to the environment population in order of increasing
              difficulty. In the
              population, the first environment $\mathcal{X}_{0}$ is always the most simple one, i.e., the one without
              obstacles.
              In order to decide when to generate the next environment, we pair the first environment $\mathcal{X}_{0}$
              from the
              population $\mathcal{X}$ with the agent $\mathcal{Y}$ and optimize the agent's behavior in that
              environment until it
              reaches a predefined success rate. After satisfying the success rate, a the new environment
              $\mathcal{X}_{1}$,
              slightly harder than the previous one $\mathcal{X}_{0}$is generated (e.g., by adding obstacles and/or
              changing their
              positions or size). In principle, this process could be continued in an open-ended manner, i.e., without
              specific
              bounds. And as a result, we could continuously create ever more challenging environments, each one
              originating from
              the previous one, and the training could continue indefinitely. However, for practical experimental
              reasons we set
              an upper bound ($E$) to limit the maximum number of environments.

              With this approach, the agent seeks to solve the newly generated environments by utilizing its existing
              skills,
              which are acquired from the previous environments. In this way, the agent transfers and adapts its
              existing behavior
              to the new environment.
              Note that in the original POET the newly generated environments are not added to the current
              population if they are either too hard or too easy for the current population of agents. However, testing
              whether an
              environment is too hard or too easy in our case incurs an excessive computational cost, due to the
              complexity of
              task at hand. Moreover, we must ensure that the agents attain the predefined success rate in the current
              environment
              before solving the next one

              Algorithm describes our method in the form of pseudo-code. As shown in the pseudo-code, we start with a
              very simple
              environment and train it using the HER framework. When the performance becomes greater than or equal to
              the
              predefined success rate $\delta$, the next (more challenging) environment is created and the agent tries
              to solve
              the new environment with its current skills. Success is defined as reaching a target position within a
              distance set
              by a threshold $\epsilon_{R}$. After each episode, we run a predefined number of test rollouts
              ($n_{test-rollouts}$)
              with the current policy and calculate the success rate $\delta$ based on how many rollouts out of
              $n_{test-rollouts}$ succeeded in the task.

            </p>

            <tbody>
              <tr>
                <td width="1200px">
                  <center><a href="./web/images/algorithm_1.png"><img src="./web/images/algorithm_1.png"
                        height="500px"></a><br></center>
                </td>
              </tr>
            </tbody>
          </div>
        </div>
      </div>
      <!--/ Proposed method. -->
      <!-- Experiments. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experiments</h2>
          <div class="content has-text-justified">
            <p>
              We conduct experiments on the MuJoCo simulation environments provided by OpenAI Gym and used as standard
              benchmark
              for multi-goal RL. Two standard manipulation tasks, both based on a 7-DOF fetch robotic manipulator, are
              chosen,
              namely pick-and-place and fetch-push. Because the environments may be
              generated in an open-ended way but training is computationally expensive, we limit the maximum number of
              environments ($E$) to $4$ for both tasks. That allows us to train on both tasks multiple times to prove
              our concept
              and provide statistics.

              \textbf{PickAndPlace (FrankaPickAndPlace-v1)}:
              The pick-and-place task with $4$ different environments is shown in Fig.1. The
              objective is to grasp the cube and bring it to the target position. The cube is shown as a black box, and
              its
              initial position is sampled uniformly within the yellow area. The target is the red dot, which is sampled
              uniformly
              within the blue region. The obstacles are colored in magenta. The task's difficulty is gradually increased
              by adding
              fixed blocks to the different locations on the table, and four different environments are generated in
              total. In the
              first environment, shown in Fig. 1, the robot learns how to pick up the cube and place it on
              the target position. In the second environment, shown in Fig. 1, an obstacle with 0.2m width,
              0.02m depth, and 0.5m height is placed on the other side of the robot on the table. In the third
              environment, shown
              in Fig. 1, another obstacle with 0.3m width, 0.02m depth, and 0.3m height is placed. In the
              last environment, shown in Fig. 1, an obstacle with 0.2m width, 0.02m depth, and 0.9m height
              is placed in front of the target sampled area.

              \textbf{FetchPush (FetchPush-v1)}:
              A fetch-push task with $4$ different environments is shown in Fig. 2.
              A cube (the black box) and a target (the red dot) are sampled uniformly within the yellow and blue areas,
              respectively. The objective is to push the cube into the target position with a clamped gripper. The
              task's
              difficulty is gradually increased by adding fixed obstacles (colored in magenta) at different locations on
              the
              table, and also in this case four different environments are generated in total. In the first environment,
              shown in
              Fig. 2, the robot learns how to push the cube to the target point. In the second environment, shown
              in Fig. 2, the robot needs to adapt its learned policy from the previous environment to avoid the
              obstacle. In the third environment, shown in Fig. 2, there is only a 10cm gap within the two
              obstacles, and the robot should push the cube through this gap. Another obstacle is placed in the middle
              of the
              table in the fourth environment shown in Fig. 2, and the robot must avoid it in order to reach the
              target position.

              In both tasks, the state is a vector consisting of the position, orientation, linear velocity, and angular
              velocity
              of the robot's end-effector, as well as the position of the cube and target. Note that the information
              about the
              obstacles is not included in the state vector. It is assumed that the task is accomplished if cube reaches
              the goal
              within a given distance threshold , in which case it receives a non-negative
              reward $0$.

              We compare the performance of our framework (COHER) against vanilla HER, HGG and CHER. During training
              with COHER,
              environments co-adapt with the agent. When the current environment $\mathcal{X}_n$ performance reaches the
              predefined success rate $\delta$, the next environment $\mathcal{X}_{n+1}$ is selected and the agent tries
              to solve
              the new environment with its learned model. On the other hand, HER, HGG and CHER are trained directly on
              the last
              (i.e., the most difficult) environment in the population considered in COHER. Our goal is to demonstrate
              how the
              co-adapting training method accelerates learning.


              PickAndPlace and FetchPush are run with 20 and 40 different seeds (The number of runs is different for the
              two tasks due to limitations on the computational resources.), respectively, on a single machine with one
              CPU core.
              The success rate $\delta$ is chosen as $0.9$ and $0.7$. As the outcomes of each episode can be influenced
              by
              multiple random factors, the agent completes the task by using a different number of episodes at each run.
              Therefore, we illustrate the worst-case training results of COHER and HER and the best-case training of
              HGG and CHER
              among the different training results. These training results are illustrated in Fig.
              3 for the PickAndPlace task and in Fig.
              4 for the FetchPush task.
              It can be seen that COHER requires 54400 and 34550 episodes to complete the task, respectively for
              PickAndPlace
              and FetchPush.

              Concerning PickAndPlace, the first environment takes 24850 episodes, while the second and third
              environments are
              generated at 33800 and 46700 episodes, respectively. In other words, 8950 and 12900 episodes are required
              to
              reach the given success rate for them.

              As for FetchPush, solving the first environment takes 8950 episodes, while the second and third
              environments are
              generated at 12150 and 16550 episodes, respectively. In other words, 3200 and 4400 episodes are required
              to
              reach the given success rate for them.

              Compared to COHER, HER requires 223550 and 68200 episodes to reach the same success rate respectively for
              PickAndPlace and FetchPush. On the other hand, HGG and CHER get stuck most of the time in the presence of
              obstacles,
              because as discussed earlier their heuristic method for generating curriculum is based on Euclidean
              distance. For
              PickAndPlace in particular, HGG's success rate is always 0.

              The environment transition points are depicted as orange, brown, and purple dots in Fig.
              3 and Fig. 4 respectively for the
              two tasks. It can be seen that, with COHER, the performance drops as soon as the next challenging
              environment is
              generated, but the RL algorithm adapts itself to the new environment until it reaches the success rate.

              The total number of episodes required to complete the two tasks in different runs is shown as a box plot
              in Fig.
              3 and Fig. 4.
              The mean and median values are shown as a red dashed line and a solid green line, respectively. The
              corresponding
              numerical values are reported in Table .


              Fig. 3 and Fig. 4 show the number of
              episodes required for each environment in order to reach the predefined success rate. On average, the
              environments
              require 6411.29, 2575.81, 7948.39, and 5216.13 episodes for PickAndPlace and 5810.25, 2332.05, 3453.85,
              and 9741.03
              episodes for FetchPush . Moreover, the figures shed light on the difficulty level of each environment.
              Since the
              robot starts in the first environment without knowing anything about the task, it takes on average a
              little bit
              longer than the second environment. In the second environment, the obstacle is located on the other side
              of the
              robot arm, and the location of the obstacle does not intersect with the sampled area of the initial
              position (i.e.,
              the yellow area) of the cube. As a result, the robot can easily apply the skills it learned in the first
              environment. After the robot succeeds in the second environment, it learns to avoid the obstacle either by
              pushing
              the cube around it or by moving the cube above it, depending on the task. When the third environment is
              introduced,
              the robot arm is blocked more often than in the second environment. The reason is that the third
              environment has a
              much smaller gap than the second one, and also that the robot has just learned to go through the safe way,
              reaching
              the goal on the other side of the obstacle in the second environment, but now another obstacle is located
              on its
              safely learned path.

              As for the last environment, the obstacle is located in the middle of the table for FetchPush and on the
              left side
              of the table for PickAndPlace. The last environment for FetchPush takes the longest to be solved, because
              a newly
              located obstacle intersects with the sampled area of the target. Furthermore, the robot needs to push the
              cube
              around it and bring it to the target point.
            </p>

            <img src="./web/images/frankapickandplace_env_fig.png" alt="frankapickandplace_result"
              style="width:100%">
            <img src="./web/images/fetchpush_env_figure.png" alt="fetchpush_result" style="width:100%">
            <div class="row_mujoco_robotics">
              <div class="column_mujoco_robotics">
                <img src="./web/images/sim2real_obstacle_measurement.png" alt="fethcpickandplace_env0"
                  style="width:100%">
              </div>
              <div class="column_mujoco_robotics">
                <img src="./web/images/sim2real_setup.png" alt="fethcpickandplace_env1" style="width:100%">
              </div>
            </div>
          </div>
          <img src="./web/images/frankapickandplace_result.png" alt="frankapickandplace_result" style="width:100%">
          <img src="./web/images/fetchpush_result.png" alt="fetchpush_result" style="width:100%">
        </div>
        <!--/ Experiments. -->

      </div>
      <div style="display: flex; justify-content: space-evenly;">
        <table class="table_robotic_tasks">
          <thead>
            <tr>
              <th>FrankaPickAndPlace</th>
              <th>COHER</th>
              <th>HER</th>
            </tr>
          </thead>
          <tbody>
            <tr class="active-row">
              <td>mean</td>
              <td>53850</td>
              <td>53850</td>
            </tr>
            <tr class="active-row">
              <td>median</td>
              <td>52600</td>
              <td>52600</td>
            </tr>
            <tr class="active-row">
              <td>standard deviation</td>
              <td>294900</td>
              <td>294900</td>
            </tr>
          </tbody>
        </table>

        <table class="table_robotic_tasks">
          <thead>
            <tr>
              <th>FetchPush</th>
              <th>COHER</th>
              <th>HER</th>
            </tr>
          </thead>
          <tbody>
            <tr class="active-row">
              <td>mean</td>
              <td>53850</td>
              <td>53850</td>
            </tr>
            <tr class="active-row">
              <td>median</td>
              <td>52600</td>
              <td>52600</td>
            </tr>
            <tr class="active-row">
              <td>standard deviation</td>
              <td>294900</td>
              <td>294900</td>
            </tr>
          </tbody>
        </table>
      </div>
    <h3>How to run benchmarks</h3>
    <div class="highlight1">
      <pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#fd00dc">#!/bin/bash</span><br><span style="color:#423004">Parameters can be changed using params.yaml file in the repo before starting the experiment</span>
      </br><span style="color:#0167ff">docker-compose run --rm coher</span>
    </code></pre>
    </div>
  </section>



  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
    </div>
  </section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://ieeexplore.ieee.org/document/10480429">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/erdiphd" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
                The <a href="https://github.com/nerfies/nerfies.github.io">source code</a> for the website 
                was taken from Nerfies. We appreciate the authors sharing the templates with us.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>